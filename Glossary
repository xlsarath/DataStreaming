Stream - An unbounded sequence of ordered, immutable data
Stream Processing - Continual calculations performed on one or more Streams
Immutable Data - Data that cannot be changed once it has been created
Event - An immutable fact regarding something that has occurred in our system.
Batch Processing - Scheduled, periodic analysis of one or more groups of related data.
Data Store - A generic place that holds data of some kind, like a message queue or data store
Stream Processing Application - An application which is downstream of one or more data streams and performs some kind of calculation on incoming data, typically producing one or more output data streams
Stream Processing Framework - A set of tools, typically bundled as a library, used to construct a Stream Processing Application
Real-time - In relation to processing, this implies that a piece of data, or an event, is processed almost as soon as it is produced. Strict time-based definitions of real-time are controversial in the industry and vary widely between applications. For example, a Computer Vision application may consider real-time to be 1 millisecond or less, whereas a data engineering team may consider it to be 30 seconds or less. In this class when the term "real-time" is used, the time-frame we have in mind is seconds.
Append-only Log - files in which incoming events are written to the end of the file as they are received
Change Data Capture (CDC) - The process of capturing change events, typically in SQL database systems, in order to accurately communicate and synchronize changes from primary to replica nodes in a clustered system.
Log-Structured Storage - Systems built on Append-Only Logs, in which system data is stored in log format.
Merge (Log Files) - When two or more log files are joined together into a single output log file
Compact (Log Files) - When data from one or more files is deleted, typically based on the age of data
Source (Kafka) - A term sometimes used to refer to Kafka clients which are producing data into Kafka, typically in reference to another data store
Sink (Kafka) - A term sometimes used to refer to Kafka clients which are extracting data from Kafka, typically in reference to another data store
Topic (Kafka) - A logical construct used to organize and segment datasets within Kafka, similar to how SQL databases use tables
Producer (Kafka) - An application which is sending data to one or more Kafka Topics.
Consumer (Kafka) - An application which is receiving data from one or more Kafka Topics.
Glossary of Key Terms you Will Learn in this Lesson:
Broker (Kafka) - A single member server of the Kafka cluster
Cluster (Kafka) - A group of one or more Kafka Brokers working together to satisfy Kafka production and consumption
Node - A single computing instance. May be physical, as in a server in a datacenter, or virtual, as an instance might be in AWS, GCP, or Azure.
Zookeeper - Used by Kafka Brokers to determine which broker is the leader of a given partition and topic, as well as track cluster membership and configuration for Kafka
Access Control List (ACL) - Permissions associated with an object. In Kafka, this typically refers to a userâ€™s permissions with respect to production and consumption, and/or the topics themselves.
JVM - The Java Virtual Machine - Responsible for allowing host computers to execute the byte-code compiled against the JVM.
Data Partition (Kafka) - Kafka topics consist of one or more partitions. A partition is a log which provides ordering guarantees for all of the data contained within it. Partitions are chosen by hashing key values.
Data Replication (Kafka) - A mechanism by which data is written to more than one broker to ensure that if a single broker is lost, a replicated copy of the data is available.
In-Sync Replica (ISR) - A broker which is up to date with the leader for a particular broker for all of the messages in the current topic. This number may be less than the replication factor for a topic.
Rebalance - A process in which the current set of consumers changes (addition or removal of consumer). When this occurs, assignment of partitions to the various consumers in a consumer group must be changed.
Data Expiration - A process in which data is removed from a Topic log, determined by data retention policies.
Data Retention - Policies that determine how long data should be kept. Configured by time or size.
Batch Size - The number of messages that are sent or received from Kafka
acks - The number of broker acknowledgements that must be received from Kafka before a producer continues processing
Synchronous Production - Producers which send a message and wait for a response before performing additional processing
Asynchronous Production - Producers which send a message and do not wait for a response before performing additional processing
Avro - A binary message serialization format
Message Serialization - The process of transforming an applications internal data representation to a format suitable for interprocess communication over a protocol like TCP or HTTP.
Message Deserialization - The process of transforming an incoming set of data from a form suitable for interprocess communication, into a data representation more suitable for the application receiving the data.
Retries (Kafka Producer) - The number of times the underlying library will attempt to deliver data before moving on
Consumer Offset - A value indicating the last seen and processed message of a given consumer, by ID.
Consumer Group - A collection of one or more consumers, identified by group.id which collaborate to consume data from Kafka and share a consumer offset.
Consumer Group Coordinator - The broker in charge of working with the Consumer Group Leader to initiate a rebalance
Consumer Group Leader - The consumer in charge of working with the Group Coordinator to manage the consumer group
Topic Subscription - Kafka consumers indicate to the Kafka Cluster that they would like to consume from one or more topics by specifying one or more topics that they wish to subscribe to.
Consumer Lag - The difference between the offset of a consumer group and the latest message offset in Kafka itself
CCPA - California Consumer Privacy Act
GDPR - General Data Protection Regulation
when network overhead is critical : zstd compression is preferred 
lz4 is preferred when decompression speed is critical
when ack is -1 brokers need to acknowledge the message before proceeding
Glossary of Key Terms You Will Learn in this Lesson
Data Schema - Define the shape of a particular kind of data. Specifically, data schemas define the expected fields, their names, and value types for those fields. Data schemas may also indicate whether fields are required or optional.
Apache Avro - A data serialization framework which includes facilities for defining and communicating data schemas. Avro is widely used in the Kafka ecosystem and data engineering generally.
Record (Avro) - A single encoded record in the defined Avro format
Primitive Type (Avro) - In Avro, a primitive type is a type which requires no additional specification - null, boolean, int, long, float, double, bytes, string.
Complex Type (Avro) - In Avro, a complex type models data structures which may involve nesting or other advanced functionality: records, enums, maps, arrays, unions, fixed.
Schema Evolution - The process of modifying an existing schema with new, deleted, or modified fields.
Schema Compatibility - Determines whether or not two given versions of a schema are usable by a given client
Backward Compatibility - means that consumer code developed against the most recent version of an Avro Schema can use data using the prior version of a schema without modification.
Forward Compatibility - means that consumer code developed against the previous version of an Avro Schema can consume data using the newest version of a schema without modification.
Full Compatibility - means that consumers developed against the latest schema can consume data using the previous schema, and that consumers developed against the previous schema can consume data from the latest schema as well. In other words, full compatibility means that a schema change is both forward and backward compatible.
None Compatibility - disables compatibility checking by Schema Registry.
The process of changing the definition of our data schema is known as Schema Evolution.
Schema Registry can help keep track of schema changes and their compatibility with existing schemas using its compatibility API.
BACKWARD and BACKWARD_TRANSITIVE compatibility indicate that consumers developed against the latest version of the schema can use one or more previous schemas. BACKWARD is the default for Schema Registry.
FORWARD and FORWARD_TRANSITIVE compatibility indicate that consumers developed against the previous (or potentially earlier) version of a schema can continue to use data with the latest schema definition.
FULL and FULL_TRANSITIVE compatibility indicate that a schema change is both forward and backward compatible.
NONE compatibility means that compatibility is not tracked. Use of NONE is not recommended.
